{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "\n",
    "# Join Orders com Customers\n",
    "orders_df = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/orders', format='delta')\n",
    "customers_df = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/customers', format='delta')\n",
    "\n",
    "orders_customers_df = orders_df.join(customers_df, orders_df.customerId == customers_df.customerId, \"left\").drop(customers_df.customerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "\n",
    "# Join OrdersItems com Orders e Products\n",
    "orders_items_df = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/orderItems', format='delta')\n",
    "products_df = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/products', format='delta')\n",
    "\n",
    "orders_items_orders_df = orders_items_df.join(orders_df, orders_items_df.orderId == orders_df.orderId, \"left\").drop(orders_df.orderId)\n",
    "orders_items_orders_products_df = orders_items_orders_df.join(products_df, orders_items_orders_df.productId == products_df.productId, \"left\").drop(products_df.productId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "\n",
    "# Join Employees com Departments\n",
    "employees_df = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/employees', format='delta')\n",
    "departments_df = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/departments', format='delta')\n",
    "\n",
    "employees_departments_df = employees_df.join(departments_df, employees_df.departmentId == departments_df.departmentId, \"left\").drop(departments_df.departmentId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "\n",
    "# Combinar todas as tabelas em uma única tabela\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Adicionar um identificador de tipo para cada DataFrame\n",
    "orders_customers_df = orders_customers_df.withColumn(\"type\", lit(\"order_customer\"))\n",
    "orders_items_orders_products_df = orders_items_orders_products_df.withColumn(\"type\", lit(\"order_item\"))\n",
    "employees_departments_df = employees_departments_df.withColumn(\"type\", lit(\"employee\"))\n",
    "\n",
    "# Selecionar colunas comuns para união\n",
    "orders_customers_common_df = orders_customers_df.select(\"type\", \"orderId\", \"customerId\", \"createdAt\", \"total\", \"name\", \"address\", \"email\")\n",
    "orders_items_orders_products_common_df = orders_items_orders_products_df.select(\"type\", \"itemId\", \"orderId\", \"productId\", \"productName\", \"quantity\", \"unitPrice\", \"totalPrice\")\n",
    "employees_departments_common_df = employees_departments_df.select(\"type\", \"employeeId\", \"name\", \"position\", \"createdAt\", \"salary\", \"departmentId\", \"location\")\n",
    "\n",
    "# Realizar a união\n",
    "final_df = orders_customers_common_df.unionByName(orders_items_orders_products_common_df, allowMissingColumns=True)\n",
    "final_df = final_df.unionByName(employees_departments_common_df, allowMissingColumns=True)\n",
    "\n",
    "# Salvar o resultado em um arquivo Parquet\n",
    "output_path = 'abfss://gold@datalakeengdados.dfs.core.windows.net/OneBigTable.parquet'\n",
    "final_df.write.mode('overwrite').delta(output_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
