{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto de Engenharia de Dados","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o do nosso projeto de engenharia de dados. Este projeto envolve a utiliza\u00e7\u00e3o do MongoDB para exporta\u00e7\u00e3o de dados para o Azure Synapse, processamento de dados em v\u00e1rias camadas (Landing, Bronze, Silver, Gold) e visualiza\u00e7\u00e3o de informa\u00e7\u00f5es no Power BI.</p>"},{"location":"#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Neste projeto, seguimos um fluxo de dados estruturado em v\u00e1rias camadas para garantir a qualidade e a integridade dos dados antes de apresent\u00e1-los no Power BI. Abaixo est\u00e1 uma vis\u00e3o geral de cada componente envolvido no processo.</p>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>MongoDB: Utilizado para armazenamento de dados.</li> <li>Azure Synapse: Utilizado para processamento e an\u00e1lise de dados.</li> <li>Power BI: Utilizado para visualiza\u00e7\u00e3o de dados.</li> <li>MkDocs: Utilizado para esta documenta\u00e7\u00e3o.</li> </ul>"},{"location":"#estrutura-do-projeto","title":"Estrutura do Projeto","text":""},{"location":"#1-camada-landing","title":"1. Camada Landing","text":"<p>A camada de landing \u00e9 onde os dados s\u00e3o inicialmente armazenados quando recebidos de diferentes fontes. Aqui, mantemos os dados em seu formato bruto e original.</p>"},{"location":"#2-camada-bronze","title":"2. Camada Bronze","text":"<p>Na camada bronze, os dados brutos s\u00e3o transformados em um formato mais estruturado, mas ainda cont\u00eam alguns erros e duplicatas.</p>"},{"location":"#3-camada-silver","title":"3. Camada Silver","text":"<p>A camada silver \u00e9 onde os dados s\u00e3o limpos e preparados para an\u00e1lises mais detalhadas. Nesta etapa, removemos duplicatas e corrigimos erros.</p>"},{"location":"#4-camada-gold","title":"4. Camada Gold","text":"<p>Na camada gold, os dados s\u00e3o refinados e otimizados para o consumo final por parte de sistemas de BI, como o Power BI. Esses dados s\u00e3o altamente estruturados e prontos para an\u00e1lises detalhadas.</p>"},{"location":"#fluxo-de-trabalho","title":"Fluxo de Trabalho","text":"<ol> <li>Ingest\u00e3o de Dados no MongoDB: Os dados s\u00e3o coletados de v\u00e1rias fontes e armazenados no MongoDB.</li> <li>Exporta\u00e7\u00e3o para o Azure Synapse: Os dados s\u00e3o exportados do MongoDB para o Azure Synapse para processamento adicional.</li> <li>Processamento em Camadas:</li> <li>Landing: Armazenamento de dados brutos.</li> <li>Bronze: Transforma\u00e7\u00e3o inicial e limpeza b\u00e1sica.</li> <li>Silver: Limpeza e prepara\u00e7\u00e3o avan\u00e7ada.</li> <li>Gold: Refino e otimiza\u00e7\u00e3o final.</li> <li>Visualiza\u00e7\u00e3o no Power BI: Os dados refinados s\u00e3o carregados no Power BI para cria\u00e7\u00e3o de dashboards e relat\u00f3rios interativos.</li> </ol>"},{"location":"#navegacao","title":"Navega\u00e7\u00e3o","text":"<ul> <li>MongoDB</li> <li>Azure Synapse</li> <li>Landing</li> <li>Bronze</li> <li>Silver</li> <li>Gold</li> <li>Power BI</li> </ul>"},{"location":"#contato","title":"Contato","text":"<p>Para mais informa\u00e7\u00f5es, entre em contato com nossa equipe atrav\u00e9s do e-mail: email@example.com.</p> <p>Documenta\u00e7\u00e3o gerada com MkDocs.</p>"},{"location":"azure_synapse/","title":"Azure Synapse","text":""},{"location":"azure_synapse/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Azure Synapse \u00e9 utilizado para processar e analisar grandes volumes de dados. Ele oferece um ambiente unificado para ingest\u00e3o, prepara\u00e7\u00e3o, gerenciamento e entrega de dados para BI e machine learning.</p>"},{"location":"azure_synapse/#objetivo","title":"Objetivo","text":"<p>O objetivo do Azure Synapse no projeto \u00e9 fornecer uma plataforma robusta para processamento e transforma\u00e7\u00e3o de dados, al\u00e9m de armazenar os dados processados em diferentes camadas (Bronze, Silver, Gold).</p>"},{"location":"azure_synapse/#processos","title":"Processos","text":""},{"location":"azure_synapse/#pipeline-de-dados","title":"Pipeline de Dados","text":"<ol> <li>Ingest\u00e3o de Dados: Dados brutos s\u00e3o exportados do MongoDB e ingeridos no Azure Synapse.</li> <li>Transforma\u00e7\u00e3o Inicial (Bronze): Convers\u00e3o de dados brutos em formatos estruturados.</li> <li>Limpeza de Dados (Silver): Remo\u00e7\u00e3o de duplicatas, corre\u00e7\u00e3o de erros e preenchimento de valores ausentes.</li> <li>Refinamento de Dados (Gold): Cria\u00e7\u00e3o de m\u00e9tricas e agrega\u00e7\u00f5es para an\u00e1lise final.</li> </ol>"},{"location":"azure_synapse/#ferramentas-utilizadas","title":"Ferramentas Utilizadas","text":"<ul> <li>Azure Synapse Studio: Ambiente para desenvolvimento e gest\u00e3o de pipelines de dados.</li> <li>Apache Spark: Para processamento de dados em grande escala.</li> <li>SQL: Para consultas e transforma\u00e7\u00f5es de dados.</li> </ul> <p>Voltar para a p\u00e1gina inicial</p>"},{"location":"bronze/","title":"Camada Bronze","text":""},{"location":"bronze/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Na camada Bronze, os dados brutos da camada Landing s\u00e3o transformados em um formato mais estruturado. Nesta etapa, os dados ainda podem conter duplicatas e erros, mas est\u00e3o mais organizados.</p>"},{"location":"bronze/#objetivo","title":"Objetivo","text":"<p>O objetivo da camada Bronze \u00e9 realizar uma transforma\u00e7\u00e3o inicial nos dados brutos para facilitar o processamento posterior.</p>"},{"location":"bronze/#codigo-utf-8","title":"C\u00f3digo: utf-8","text":""},{"location":"bronze/#bronze","title":"Bronze","text":""},{"location":"bronze/#listando-todos-os-arquivos-da-camada-landing-zone","title":"Listando todos os arquivos da camada Landing-Zone","text":"<pre><code>landing_zone_path = \"abfss://landing-zone@datalakeengdados.dfs.core.windows.net/\"\n\ndf = spark.read.format(\"binaryFile\").load(landing_zone_path)\n\nfile_paths = df.select(\"path\").collect()\nfor file in file_paths:\n    print(file[\"path\"])\n</code></pre>"},{"location":"bronze/#products","title":"Products","text":""},{"location":"bronze/#gerando-um-dataframe-products","title":"Gerando um dataframe (Products)","text":"<pre><code>df_products = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/products.parquet', format='parquet')\ndf_products.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-products","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (Products)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_products = df_products.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_products = df_products.withColumn(\"nome_arquivo\", lit(\"products.parquet\"))\ndf_products.printSchema()\ndf_products.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta","title":"Salvando na camada Bronze com o formato Delta","text":"<pre><code>bronze_products = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/products'\n\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_products)\n</code></pre>"},{"location":"bronze/#customers","title":"Customers","text":""},{"location":"bronze/#gerando-um-dataframe-customers","title":"Gerando um dataframe (Customers)","text":"<pre><code>df_customers = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/customers.parquet', format='parquet')\ndf_customers.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-customers","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (Customers)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_customers = df_customers.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_customers = df_customers.withColumn(\"nome_arquivo\", lit(\"customers.parquet\"))\ndf_customers.printSchema()\ndf_customers.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta-customers","title":"Salvando na camada Bronze com o formato Delta (Customers)","text":"<pre><code>bronze_customers = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/customers'\n\ndf_customers.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_customers)\n</code></pre>"},{"location":"bronze/#departments","title":"Departments","text":""},{"location":"bronze/#gerando-um-dataframe-departments","title":"Gerando um dataframe (Departments)","text":"<pre><code>df_departments = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/departments.parquet', format='parquet')\ndf_departments.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-departments","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (Departments)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_departments = df_departments.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_departments = df_departments.withColumn(\"nome_arquivo\", lit(\"departments.parquet\"))\ndf_departments.printSchema()\ndf_departments.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta-departments","title":"Salvando na camada Bronze com o formato Delta (Departments)","text":"<pre><code>bronze_departments = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/departments'\n\ndf_departments.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_departments)\n</code></pre>"},{"location":"bronze/#employees","title":"Employees","text":""},{"location":"bronze/#gerando-um-dataframe-employees","title":"Gerando um dataframe (Employees)","text":"<pre><code>df_employees = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/employees.parquet', format='parquet')\ndf_employees.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-employees","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (Employees)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_employees = df_employees.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_employees = df_employees.withColumn(\"nome_arquivo\", lit(\"employees.parquet\"))\ndf_employees.printSchema()\ndf_employees.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta-employees","title":"Salvando na camada Bronze com o formato Delta (Employees)","text":"<pre><code>bronze_employees = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/employees'\n\ndf_employees.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_employees)\n</code></pre>"},{"location":"bronze/#orderitems","title":"OrderItems","text":""},{"location":"bronze/#gerando-um-dataframe-orderitems","title":"Gerando um dataframe (OrderItems)","text":"<pre><code>df_orderItems = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/orderItems.parquet', format='parquet')\ndf_orderItems.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-orderitems","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (OrderItems)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_orderItems = df_orderItems.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_orderItems = df_orderItems.withColumn(\"nome_arquivo\", lit(\"orderItems.parquet\"))\ndf_orderItems.printSchema()\ndf_orderItems.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta-orderitems","title":"Salvando na camada Bronze com o formato Delta (OrderItems)","text":"<pre><code>bronze_orderItems = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/orderItems'\n\ndf_orderItems.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_orderItems)\n</code></pre>"},{"location":"bronze/#orders","title":"Orders","text":""},{"location":"bronze/#gerando-um-dataframe-orders","title":"Gerando um dataframe (Orders)","text":"<pre><code>df_orders = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/orders.parquet', format='parquet')\ndf_orders.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-orders","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (Orders)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_orders = df_orders.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_orders = df_orders.withColumn(\"nome_arquivo\", lit(\"orders.parquet\"))\ndf_orders.printSchema()\ndf_orders.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta-orders","title":"Salvando na camada Bronze com o formato Delta (Orders)","text":"<pre><code>bronze_orders = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/orders'\n\ndf_orders.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_orders)\n</code></pre>"},{"location":"bronze/#departmentproducts","title":"DepartmentProducts","text":""},{"location":"bronze/#gerando-um-dataframe-departmentproducts","title":"Gerando um dataframe (DepartmentProducts)","text":"<pre><code>df_departmentProducts = spark.read.load('abfss://landing-zone@datalakeengdados.dfs.core.windows.net/departmentProducts.parquet', format='parquet')\ndf_departmentProducts.printSchema()\n</code></pre>"},{"location":"bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem-departmentproducts","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem (DepartmentProducts)","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_departmentProducts = df_departmentProducts.withColumn(\"data_hora_bronze\", current_timestamp())\ndf_departmentProducts = df_departmentProducts.withColumn(\"nome_arquivo\", lit(\"departmentProducts.parquet\"))\ndf_departmentProducts.printSchema()\ndf_departmentProducts.show(10)\n</code></pre>"},{"location":"bronze/#salvando-na-camada-bronze-com-o-formato-delta-departmentproducts","title":"Salvando na camada Bronze com o formato Delta (DepartmentProducts)","text":"<pre><code>bronze_departmentProducts = 'abfss://bronze@datalakeengdados.dfs.core.windows.net/departmentProducts'\n\ndf_departmentProducts.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_departmentProducts)\n</code></pre>"},{"location":"bronze/#validando-a-existencia-dos-dados-e-das-colunas-dos-metadados-no-delta-lake","title":"Validando a exist\u00eancia dos dados e das colunas dos metadados no Delta Lake","text":"<pre><code>df_products = spark.read.load('abfss://bronze@datalakeengdados.dfs.core.windows.net/products', format='delta')\ndisplay(df_products.limit(2))\n</code></pre> <p>Voltar para a p\u00e1gina inicial</p>"},{"location":"gold/","title":"Camada Gold","text":""},{"location":"gold/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>A camada Gold \u00e9 a etapa final do nosso pipeline de dados, onde os dados s\u00e3o refinados e otimizados para o consumo final por parte de sistemas de BI, como o Power BI.</p>"},{"location":"gold/#objetivo","title":"Objetivo","text":"<p>O objetivo da camada Gold \u00e9 fornecer dados altamente estruturados e otimizados para an\u00e1lises detalhadas e cria\u00e7\u00e3o de relat\u00f3rios.</p>"},{"location":"gold/#processos","title":"Processos","text":"<ol> <li> <p>Refinamento de Dados:</p> <ul> <li>Agrega\u00e7\u00e3o de dados.</li> <li>Cria\u00e7\u00e3o de m\u00e9tricas e indicadores chave.</li> <li>Aplica\u00e7\u00e3o de transforma\u00e7\u00f5es finais.</li> </ul> </li> <li> <p>Armazenamento:</p> <ul> <li>Os dados refinados s\u00e3o armazenados no Azure Synapse em tabelas otimizadas para consumo por ferramentas de BI.</li> </ul> </li> </ol>"},{"location":"gold/#estrutura-de-dados","title":"Estrutura de Dados","text":""},{"location":"gold/#visualizacao-no-power-bi","title":"Visualiza\u00e7\u00e3o no Power BI","text":"<p>Os dados refinados na camada Gold s\u00e3o carregados no Power BI para cria\u00e7\u00e3o de dashboards e relat\u00f3rios interativos. Aqui est\u00e3o alguns exemplos de visualiza\u00e7\u00f5es:</p> <p>Voltar para a p\u00e1gina inicial</p>"},{"location":"landing/","title":"Camada Landing","text":""},{"location":"landing/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>A camada Landing \u00e9 a primeira etapa do nosso pipeline de dados, onde os dados brutos s\u00e3o recebidos e armazenados. Nesta camada, os dados s\u00e3o coletados de v\u00e1rias fontes e mantidos em seu formato original, sem nenhuma transforma\u00e7\u00e3o ou limpeza.</p>"},{"location":"landing/#objetivo","title":"Objetivo","text":"<p>O objetivo da camada Landing \u00e9 garantir que todos os dados recebidos sejam armazenados de forma segura e \u00edntegra para processamento posterior.</p>"},{"location":"landing/#processos","title":"Processos","text":"<ol> <li> <p>Ingest\u00e3o de Dados:</p> <ul> <li>Coleta de dados de v\u00e1rias fontes (APIs, arquivos CSV, bancos de dados, etc.).</li> <li>Armazenamento dos dados no MongoDB.</li> </ul> </li> <li> <p>Armazenamento:</p> <ul> <li>Os dados s\u00e3o armazenados no MongoDB em cole\u00e7\u00f5es espec\u00edficas para cada fonte de dados.</li> </ul> </li> </ol>"},{"location":"landing/#estrutura-de-dados","title":"Estrutura de Dados","text":""},{"location":"landing/#ferramentas-utilizadas","title":"Ferramentas Utilizadas","text":"<ul> <li>MongoDB: Para armazenamento dos dados brutos.</li> <li>Scripts de Ingest\u00e3o: Scripts desenvolvidos em Python para coletar e armazenar os dados.</li> </ul> <p>Voltar para a p\u00e1gina inicial</p>"},{"location":"mongodb/","title":"MongoDB","text":""},{"location":"mongodb/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>O MongoDB \u00e9 usado como a solu\u00e7\u00e3o de banco de dados NoSQL para armazenar os dados brutos que s\u00e3o recebidos de v\u00e1rias fontes. Esta camada inicial \u00e9 essencial para capturar os dados em seu formato original antes de qualquer processamento ou transforma\u00e7\u00e3o.</p>"},{"location":"mongodb/#objetivo","title":"Objetivo","text":"<p>O objetivo do MongoDB no projeto \u00e9 fornecer um armazenamento flex\u00edvel e escal\u00e1vel para os dados brutos, facilitando a ingest\u00e3o e a recupera\u00e7\u00e3o r\u00e1pida desses dados para processamento subsequente.</p>"},{"location":"mongodb/#estrutura-de-dados","title":"Estrutura de Dados","text":""},{"location":"mongodb/#colecoes","title":"Cole\u00e7\u00f5es","text":""},{"location":"mongodb/#processos","title":"Processos","text":""},{"location":"mongodb/#ingestao-de-dados","title":"Ingest\u00e3o de Dados","text":"<ol> <li>Coleta de Dados: Dados s\u00e3o coletados de v\u00e1rias fontes, como APIs, arquivos CSV, etc.</li> <li>Armazenamento no MongoDB: Os dados s\u00e3o armazenados no MongoDB em cole\u00e7\u00f5es espec\u00edficas para cada tipo de dado.</li> </ol>"},{"location":"mongodb/#consultas-de-exemplos","title":"Consultas de Exemplos","text":"<ul> <li>Consultas:</li> </ul>"},{"location":"mongodb/#ferramentas-utilizadas","title":"Ferramentas Utilizadas","text":"<ul> <li>MongoDB Atlas: Plataforma de banco de dados como servi\u00e7o (DBaaS) para MongoDB.</li> <li>MongoDB Compass: Ferramenta GUI para visualizar e explorar dados no MongoDB.</li> <li>Drivers MongoDB: Usados para conectar e interagir com o MongoDB a partir de diferentes linguagens de programa\u00e7\u00e3o.</li> </ul> <p>Voltar para a p\u00e1gina inicial</p>"},{"location":"power_bi/","title":"Power BI","text":""},{"location":"power_bi/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>O Power BI \u00e9 utilizado para visualizar e analisar os dados refinados da camada Gold. Ele permite a cria\u00e7\u00e3o de dashboards interativos e relat\u00f3rios detalhados para tomada de decis\u00e3o.</p>"},{"location":"power_bi/#objetivo","title":"Objetivo","text":"<p>O objetivo do Power BI no projeto \u00e9 fornecer uma plataforma de visualiza\u00e7\u00e3o de dados onde usu\u00e1rios podem explorar e interagir com os dados refinados para obter insights valiosos.</p>"},{"location":"power_bi/#processos","title":"Processos","text":""},{"location":"power_bi/#conexao-ao-azure-synapse","title":"Conex\u00e3o ao Azure Synapse","text":"<ol> <li> <p>Importa\u00e7\u00e3o de Dados:</p> <ul> <li>Conecte-se ao Azure Synapse diretamente do Power BI.</li> <li>Importe as tabelas refinadas da camada Gold para o Power BI.</li> </ul> </li> <li> <p>Modelagem de Dados:</p> <ul> <li>Crie relacionamentos entre tabelas.</li> <li>Defina medidas e colunas calculadas para an\u00e1lise.</li> </ul> </li> </ol>"},{"location":"power_bi/#exemplos-de-dashboards","title":"Exemplos de Dashboards","text":""},{"location":"power_bi/#dashboard-de-vendas","title":"Dashboard de Vendas","text":"<ul> <li>KPIs (Indicadores-Chave de Desempenho):     -- Exibe o totais de Lucros, Faturamento, Sal\u00e1rios e Valor m\u00e9dio de Compra.</li> <li>Gr\u00e1ficos:     -- N\u00famero deeovos clientes por ano e m\u00eas     -- Taxa de reten\u00e7\u00e3o de cliente por ano</li> </ul>"},{"location":"power_bi/#exemplos-de-visualizacoes","title":"Exemplos de Visualiza\u00e7\u00f5es","text":""},{"location":"power_bi/#ferramentas-utilizadas","title":"Ferramentas Utilizadas","text":"<ul> <li>Power BI Desktop: Para desenvolvimento de dashboards e relat\u00f3rios.</li> <li>Power BI Service: Para publica\u00e7\u00e3o e compartilhamento de dashboards e relat\u00f3rios.</li> <li>Conectores do Power BI: Para integra\u00e7\u00e3o com Azure Synapse e outras fontes de dados.</li> </ul> <p>Voltar para a p\u00e1gina inicial</p>"},{"location":"silver/","title":"Camada Silver","text":""},{"location":"silver/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>A camada Silver \u00e9 onde os dados s\u00e3o limpos e preparados para an\u00e1lises mais detalhadas. Nesta etapa, removemos duplicatas, corrigimos erros e preenchemos valores ausentes.</p>"},{"location":"silver/#objetivo","title":"Objetivo","text":"<p>O objetivo da camada Silver \u00e9 garantir que os dados estejam limpos e prontos para an\u00e1lises avan\u00e7adas.</p>"},{"location":"silver/#processos","title":"Processos","text":"<ol> <li> <p>Limpeza de Dados:</p> <ul> <li>Remo\u00e7\u00e3o de duplicatas.</li> <li>Corre\u00e7\u00e3o de valores inconsistentes.</li> <li>Preenchimento de valores ausentes.</li> </ul> </li> <li> <p>Armazenamento:</p> <ul> <li>Os dados limpos s\u00e3o armazenados no Azure Synapse em tabelas otimizadas para an\u00e1lise.</li> </ul> </li> </ol>"},{"location":"silver/#documentacao-da-camada-silver","title":"Documenta\u00e7\u00e3o da Camada Silver","text":""},{"location":"silver/#gerando-dataframes-a-partir-do-delta-lake-no-container-bronze-do-azure-data-lake-storage","title":"Gerando Dataframes a partir do Delta Lake no Container Bronze do Azure Data Lake Storage","text":""},{"location":"silver/#leitura-dos-dados","title":"Leitura dos Dados","text":"<pre><code>%%pyspark\ndf_products = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/products\")\ndf_customers = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/customers\")\ndf_departments = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/departments\")\ndf_orderItems = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/orderItems\")\ndf_orders = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/orders\")\ndf_employees = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/employees\")\ndf_departmentProducts = spark.read.format(\"delta\").load(\"abfss://bronze@datalakeengdados.dfs.core.windows.net/departmentProducts\")\n</code></pre>"},{"location":"silver/#adicionando-colunas-de-metadata","title":"Adicionando Colunas de Metadata","text":"<pre><code>%%pyspark\nfrom pyspark.sql.functions import current_timestamp, lit\n\ndf_products = df_products.withColumn(\"data_hora_silver\", current_timestamp())\ndf_products = df_products.withColumn(\"nome_arquivo\", lit(\"products\"))\n\ndf_customers = df_customers.withColumn(\"data_hora_silver\", current_timestamp())\ndf_customers = df_customers.withColumn(\"nome_arquivo\", lit(\"customers\"))\n\ndf_departments = df_departments.withColumn(\"data_hora_silver\", current_timestamp())\ndf_departments = df_departments.withColumn(\"nome_arquivo\", lit(\"departments\"))\n\ndf_orderItems = df_orderItems.withColumn(\"data_hora_silver\", current_timestamp())\ndf_orderItems = df_orderItems.withColumn(\"nome_arquivo\", lit(\"orderItems\"))\n\ndf_orders = df_orders.withColumn(\"data_hora_silver\", current_timestamp())\ndf_orders = df_orders.withColumn(\"nome_arquivo\", lit(\"orders\"))\n\ndf_employees = df_employees.withColumn(\"data_hora_silver\", current_timestamp())\ndf_employees = df_employees.withColumn(\"nome_arquivo\", lit(\"employees\"))\n</code></pre>"},{"location":"silver/#ajustando-os-nomes-das-colunas","title":"Ajustando os Nomes das Colunas","text":""},{"location":"silver/#products","title":"Products","text":"<pre><code>colunas_products = df_products.columns\nprint(colunas_products)\n\ncolunas_renomeadas_products = {\n    '$oid': 'CODIGO',\n    'productId': 'CODIGO_PRODUTO',\n    'name': 'NOME',\n    'description': 'DESCRICAO',\n    'costPrice_$numberDouble': 'PRECO_CUSTO',\n    'sellingPrice_$numberDouble': 'PRECO_VENDA',\n    '$numberInt': 'QUANTIDADE',\n    'data_hora_bronze': 'DATA_HORA_BRONZE',\n    'data_hora_silver': 'DATA_HORA_SILVER',\n    'nome_arquivo': 'NOME_ARQUIVO'\n}\n\nfor old_name, new_name in colunas_renomeadas_products.items():\n    df_products = df_products.withColumnRenamed(old_name, new_name)\n\ndisplay(df_products.limit(1))\n</code></pre>"},{"location":"silver/#salvando-dataframe-products-na-camada-silver","title":"Salvando Dataframe Products na Camada Silver","text":"<pre><code>%%pyspark\nsilver_products = 'abfss://silver@datalakeengdados.dfs.core.windows.net/products'\ndf_products.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_products)\n</code></pre>"},{"location":"silver/#customers","title":"Customers","text":"<pre><code>colunas_customers = df_customers.columns\nprint(colunas_customers)\n\ncolunas_renomeadas_customers = {\n    '$oid': 'CODIGO',\n    'customerId': 'CODIGO_CLIENTE',\n    'name': 'NOME',\n    'address': 'ENDERECO',\n    'email': 'EMAIL',\n    'data_hora_bronze': 'DATA_HORA_BRONZE',\n    'data_hora_silver': 'DATA_HORA_SILVER',\n    'nome_arquivo': 'NOME_ARQUIVO'\n}\n\nfor old_name, new_name in colunas_renomeadas_customers.items():\n    df_customers = df_customers.withColumnRenamed(old_name, new_name)\n\ndisplay(df_customers.limit(1))\n</code></pre>"},{"location":"silver/#salvando-dataframe-customers-na-camada-silver","title":"Salvando Dataframe Customers na Camada Silver","text":"<pre><code>%%pyspark\nsilver_customers = 'abfss://silver@datalakeengdados.dfs.core.windows.net/customers'\ndf_customers.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_customers)\n</code></pre>"},{"location":"silver/#employees","title":"Employees","text":"<pre><code>colunas_employees = df_employees.columns\nprint(colunas_employees)\n\ncolunas_renomeadas_employees = {\n    '$oid': 'CODIGO',\n    'employeeId': 'CODIGO_FUNCIONARIO',\n    'name': 'NOME',\n    'position': 'CARGO',\n    'createdAt': 'CADASTRADO',\n    '$numberDouble': 'SALARIO',\n    'departmentId': 'DEPARTAMENTO',\n    'data_hora_bronze': 'DATA_HORA_BRONZE',\n    'data_hora_silver': 'DATA_HORA_SILVER',\n    'nome_arquivo': 'NOME_ARQUIVO'\n}\n\nfor old_name, new_name in colunas_renomeadas_employees.items():\n    df_employees = df_employees.withColumnRenamed(old_name, new_name)\n\ndisplay(df_employees.limit(1))\n</code></pre>"},{"location":"silver/#salvando-dataframe-employees-na-camada-silver","title":"Salvando Dataframe Employees na Camada Silver","text":"<pre><code>%%pyspark\nsilver_employees = 'abfss://silver@datalakeengdados.dfs.core.windows.net/employees'\ndf_employees.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_employees)\n</code></pre>"},{"location":"silver/#departments","title":"Departments","text":"<pre><code>colunas_departments = df_departments.columns\nprint(colunas_departments)\n\ncolunas_renomeadas_departments = {\n    '$oid': 'CODIGO',\n    'departmentId': 'CODIGO_DEPARTAMENTO',\n    'name': 'NOME',\n    'location': 'LOCALIZACAO',\n    'data_hora_bronze': 'DATA_HORA_BRONZE',\n    'data_hora_silver': 'data_hora_silver',\n    'nome_arquivo': 'NOME_ARQUIVO'\n}\n\nfor old_name, new_name in colunas_renomeadas_departments.items():\n    df_departments = df_departments.withColumnRenamed(old_name, new_name)\n\ndisplay(df_departments.limit(1))\n</code></pre>"},{"location":"silver/#salvando-dataframe-departments-na-camada-silver","title":"Salvando Dataframe Departments na Camada Silver","text":"<pre><code>%%pyspark\nsilver_departments = 'abfss://silver@datalakeengdados.dfs.core.windows.net/departments'\ndf_departments.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_departments)\n</code></pre>"},{"location":"silver/#orderitems","title":"OrderItems","text":"<pre><code>colunas_orderItems = df_orderItems.columns\nprint(colunas_orderItems)\n\ncolunas_renomeadas_orderItems = {\n    '$oid': 'CODIGO',\n    'itemId': 'CODIGO_ITEM',\n    'orderId': 'CODIGO_PEDIDO',\n    'productId': 'CODIGO_PRODUTO',\n    'productName': 'NOME_DO_PRODUTO',\n    '$numberInt': 'QUANTIDADE',\n    'unitPrice_$numberDouble': 'PRECO_UNITARIO',\n    'totalPrice_$numberDouble': 'PRECO_TOTAL',\n    'data_hora_bronze': 'DATA_HORA_BRONZE',\n    'data_hora_silver': 'DATA_HORA_SILVER',\n    'nome_arquivo': 'NOME_ARQUIVO'\n}\n\nfor old_name, new_name in colunas_renomeadas_orderItems.items():\n    df_orderItems = df_orderItems.withColumnRenamed(old_name, new_name)\n\ndisplay(df_orderItems.limit(1))\n</code></pre>"},{"location":"silver/#salvando-dataframe-orderitems-na-camada-silver","title":"Salvando Dataframe OrderItems na Camada Silver","text":"<pre><code>%%pyspark\nsilver_orderItems = 'abfss://silver@datalakeengdados.dfs.core.windows.net/orderItems'\ndf_orderItems.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_orderItems)\n</code></pre>"},{"location":"silver/#orders","title":"Orders","text":"<pre><code>colunas_orders = df_orders.columns\nprint(colunas_orders)\n\ncolunas_renomeadas_orders = {\n    '$oid': 'CODIGO',\n    'orderId': 'CODIGO_PEDIDO',\n    'customerId': 'CODIGO_CLIENTE',\n    'orderDate': 'DATA_PEDIDO',\n    'totalAmount_$numberDouble': 'VALOR_TOTAL',\n    'data_hora_bronze': 'DATA_HORA_BRONZE',\n    'data_hora_silver': 'DATA_HORA_SILVER',\n    'nome_arquivo': 'NOME_ARQUIVO'\n}\n\nfor old_name, new_name in colunas_renomeadas_orders.items():\n    df_orders = df_orders.withColumnRenamed(old_name, new_name)\n\ndisplay(df_orders.limit(1))\n</code></pre>"},{"location":"silver/#salvando-dataframe-orders-na-camada-silver","title":"Salvando Dataframe Orders na Camada Silver","text":"<pre><code>%%pyspark\nsilver_orders = 'abfss://silver@datalakeengdados.dfs.core.windows.net/orders'\ndf_orders.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_orders)\n</code></pre> <p>Voltar para a p\u00e1gina inicial</p>"}]}